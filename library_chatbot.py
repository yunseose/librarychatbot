# ğŸ’¬ ê°„ë‹¨í•œ ì¼ìƒ ëŒ€í™” ì±—ë´‡
import os
import streamlit as st
import nest_asyncio

# Streamlitì—ì„œ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
nest_asyncio.apply()

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_core.output_parsers import StrOutputParser

# --- 1. Gemini API í‚¤ ì„¤ì • ---
try:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception as e:
    st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.stop()

# --- 2. LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì • (ìºì‹œ) ---
@st.cache_resource(show_spinner="ğŸ¤– ì±—ë´‡ ëª¨ë¸ ë¡œë”© ì¤‘...")
def get_chat_chain(selected_model):
    """
    LLM, í”„ë¡¬í”„íŠ¸, ì¶œë ¥ íŒŒì„œë¥¼ ê²°í•©í•œ ê¸°ë³¸ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    # LLM ë¡œë“œ
    try:
        llm = ChatGoogleGenerativeAI(
            model=selected_model,
            temperature=0.7,
            convert_system_message_to_human=True
        )
    except Exception as e:
        st.error(f"âŒ Gemini ëª¨ë¸ '{selected_model}' ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        st.info("ğŸ’¡ API í‚¤ê°€ ìœ íš¨í•œì§€, ëª¨ë¸ ì´ë¦„ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ë³´ì„¸ìš”.")
        st.stop()

    # ëŒ€í™” í”„ë¡¬í”„íŠ¸ ì„¤ì •
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ë¨¸ëŸ¬ìŠ¤í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'ì œë¯¸ë‚˜ì´'ì…ë‹ˆë‹¤. í•­ìƒ í•œêµ­ì–´ì™€ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ë©°, ëŒ€í™”ì— ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì„ì–´ ë‹µí•´ì£¼ì„¸ìš”. ğŸ¤–"),
            MessagesPlaceholder("history"), # ëŒ€í™” ê¸°ë¡ì´ ë“¤ì–´ê°ˆ ìë¦¬
            ("human", "{input}"),         # ì‚¬ìš©ìì˜ í˜„ì¬ ì…ë ¥ì´ ë“¤ì–´ê°ˆ ìë¦¬
        ]
    )
    
    # LLM ì²´ì¸ ìƒì„± (í”„ë¡¬í”„íŠ¸ | ëª¨ë¸ | ì¶œë ¥íŒŒì„œ)
    # StrOutputParser()ëŠ” LLMì˜ ì¶œë ¥(AIMessage)ì„ ê°„ë‹¨í•œ ë¬¸ìì—´(string)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    chain = prompt | llm | StrOutputParser()
    
    return chain

# --- 3. Streamlit UI ì„¤ì • ---

st.header("ë‚˜ì˜ ì¼ìƒ ëŒ€í™” ì±—ë´‡ ğŸ’¬")
st.info("Gemini ëª¨ë¸ê³¼ ììœ ë¡­ê²Œ ì¼ìƒ ëŒ€í™”ë¥¼ ë‚˜ëˆ ë³´ì„¸ìš”.")

# ì±„íŒ… ê¸°ë¡ì„ Streamlitì˜ ì„¸ì…˜ ìƒíƒœ(session_state)ì— ì €ì¥
# key="chat_messages"ëŠ” ì´ ì±„íŒ… ê¸°ë¡ì„ ì‹ë³„í•˜ëŠ” ê³ ìœ  í‚¤ì…ë‹ˆë‹¤.
chat_history = StreamlitChatMessageHistory(key="chat_messages")

# ëª¨ë¸ ì„ íƒ
option = st.selectbox("Select Gemini Model",
    ("gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash-exp"),
    index=0,
    help="ê°€ì¥ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ 2.5 Flash ëª¨ë¸ì„ ì¶”ì²œí•©ë‹ˆë‹¤."
)

# ì„ íƒëœ ëª¨ë¸ë¡œ LLM ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
simple_chain = get_chat_chain(option)

# ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•˜ëŠ” Runnable ìƒì„±
conversational_chain = RunnableWithMessageHistory(
    simple_chain,
    lambda session_id: chat_history, # session_idì— ê´€ê³„ì—†ì´ í•­ìƒ chat_history ì‚¬ìš©
    input_messages_key="input",      # í”„ë¡¬í”„íŠ¸ì˜ "{input}"ì— ì‚¬ìš©ì ì…ë ¥ì„ ë§¤í•‘
    history_messages_key="history",  # í”„ë¡¬í”„íŠ¸ì˜ "history"ì— ëŒ€í™” ê¸°ë¡ì„ ë§¤í•‘
)

# --- 4. ì±„íŒ… UI ë¡œì§ ---

# ì²« ë°©ë¬¸ ì‹œ í™˜ì˜ ë©”ì‹œì§€ ì¶”ê°€
if not chat_history.messages:
    chat_history.add_ai_message("ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ğŸ˜Š ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

# ì´ì „ ëŒ€í™” ê¸°ë¡ ëª¨ë‘ ì¶œë ¥
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
if prompt_message := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€ ì¶œë ¥
    st.chat_message("human").write(prompt_message)
    
    # AI ì‘ë‹µ ìƒì„± ë° ì¶œë ¥
    with st.chat_message("ai"):
        with st.spinner("Thinking..."):
            # config: session_idëŠ” ì•„ë¬´ ê°’ì´ë‚˜ ë„£ì–´ë„ chat_historyë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ë¨
            config = {"configurable": {"session_id": "any_id"}}
            
            # ì²´ì¸ ì‹¤í–‰ (RAGì™€ ë‹¬ë¦¬, 'context'ê°€ ì—†ëŠ” ê°„ë‹¨í•œ ë¬¸ìì—´ì„ ë°˜í™˜)
            response = conversational_chain.invoke(
                {"input": prompt_message},
                config
            )
            st.write(response)
